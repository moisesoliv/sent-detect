{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1464,
     "status": "ok",
     "timestamp": 1589382334283,
     "user": {
      "displayName": "Moisés Oliveira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3zXARzg3mEYgGoqoQsflEXnzm_wT1KuBZBsB37A=s64",
      "userId": "12199513049896561840"
     },
     "user_tz": 180
    },
    "id": "obP9rpSKlJ0k",
    "outputId": "72dd6396-aa11-4df6-f7ea-fb3901f64c90"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/tcc')\n",
    "'''\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.preprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5646,
     "status": "ok",
     "timestamp": 1589382081031,
     "user": {
      "displayName": "Moisés Oliveira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3zXARzg3mEYgGoqoQsflEXnzm_wT1KuBZBsB37A=s64",
      "userId": "12199513049896561840"
     },
     "user_tz": 180
    },
    "id": "p3WfQTxIj5gi",
    "outputId": "3137ec8e-4f4b-4439-b889-abde8a4064cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nphrase = (  \"German Chancellor Angela Merkel and US President Barack Obama \"\\n            \"converse in the Oval Office inside the White House in Washington, D.C. \"\\n            \"health care not work here @moisesoliv\")\\nprepare_text = Tokenizer()\\nwordList = prepare_text.cleanText(phrase)\\nwordList\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Teste\n",
    "'''\n",
    "phrase = (  \"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "            \"converse in the Oval Office inside the White House in Washington, D.C. \"\n",
    "            \"health care not work here @moisesoliv\")\n",
    "prepare_text = Tokenizer()\n",
    "wordList = prepare_text.cleanText(phrase)\n",
    "wordList\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1930052,
     "status": "ok",
     "timestamp": 1589384276063,
     "user": {
      "displayName": "Moisés Oliveira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3zXARzg3mEYgGoqoQsflEXnzm_wT1KuBZBsB37A=s64",
      "userId": "12199513049896561840"
     },
     "user_tz": 180
    },
    "id": "d9IAuMbPnKay",
    "outputId": "35919261-f183-46c4-b3ad-124b78cc7095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221439/221439 [32:00<00:00, 115.30it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_text = Tokenizer()\n",
    "with open('dataset/unified-dataset.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "datasets = {'text': list(),\n",
    "            'tokens': list(),\n",
    "            'source': list(),\n",
    "            'anger': list(),\n",
    "            'disgust': list(),\n",
    "            'fear': list(),\n",
    "            'joy': list(),\n",
    "            'love': list(),\n",
    "            'noemo': list(),\n",
    "            'sadness': list(),\n",
    "            'surprise': list()}\n",
    "\n",
    "for json_str in tqdm(json_list):\n",
    "    result = json.loads(json_str)\n",
    "    datasets['text'].append(result['text'])\n",
    "    datasets['tokens'].append(prepare_text.cleanText(result['text']))\n",
    "    datasets['source'].append(result['source'])\n",
    "    datasets['anger'].append(result['emotions']['anger'])\n",
    "    datasets['disgust'].append(result['emotions']['disgust'])\n",
    "    datasets['fear'].append(result['emotions']['fear'])\n",
    "    datasets['joy'].append(result['emotions']['joy'])\n",
    "    datasets['love'].append(result['emotions']['love'])\n",
    "    datasets['noemo'].append(result['emotions']['noemo'])\n",
    "    datasets['sadness'].append(result['emotions']['sadness'])\n",
    "    datasets['surprise'].append(result['emotions']['surprise'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWvxn2wlucbY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "save_object(datasets, 'dataset/organized_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7irrHcIaaJ_"
   },
   "source": [
    "## abaixo daqui nao é tão importante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3441,
     "status": "ok",
     "timestamp": 1589381409738,
     "user": {
      "displayName": "Moisés Oliveira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3zXARzg3mEYgGoqoQsflEXnzm_wT1KuBZBsB37A=s64",
      "userId": "12199513049896561840"
     },
     "user_tz": 180
    },
    "id": "Sze66f0bjows",
    "outputId": "bb89fe61-c219-4cf5-e16d-e07af7b7b6d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>noemo</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember what she said in my last letter? \"</td>\n",
       "      <td>[rememb, what, she, said, in, my, last, letter]</td>\n",
       "      <td>emobank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>@PressSec sounds so defensive  &amp;amp; combative...</td>\n",
       "      <td>[twitter_usernam, sound, so, defens, , amp, co...</td>\n",
       "      <td>grounded_emotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>[twitter_usernam, i, know, , i, wa, listenin, ...</td>\n",
       "      <td>crowdflower</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52387</th>\n",
       "      <td>I'm a #Republican, but if I have to hear my mo...</td>\n",
       "      <td>[i, 'm, a, republican, but, if, i, have, to, h...</td>\n",
       "      <td>electoraltweets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56443</th>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>[think, that, twitter_usernam, had, a, great, ...</td>\n",
       "      <td>tec</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77494</th>\n",
       "      <td>Mortar assault leaves at least 18 dead</td>\n",
       "      <td>[mortar, assault, leav, atleast18, dead]</td>\n",
       "      <td>affectivetext</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78744</th>\n",
       "      <td>He who exalts himself shall      be humbled; a...</td>\n",
       "      <td>[he, who, exalt, himself, shall, , be, humbl, ...</td>\n",
       "      <td>ssec</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83612</th>\n",
       "      <td>This did the trick : the boys now have a more ...</td>\n",
       "      <td>[thi, did, the, trick, the, boy, now, have, a,...</td>\n",
       "      <td>emotion-cause</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86026</th>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>[the, kitchen, stink]</td>\n",
       "      <td>dailydialog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189005</th>\n",
       "      <td>@ZubairSabirPTI  pls dont insult the word 'Molna'</td>\n",
       "      <td>[twitter_usernam, , pl, do, nt, insult, the, w...</td>\n",
       "      <td>emoint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>We'll be off and running to a lil' place calle...</td>\n",
       "      <td>[we, 'll, be, off, and, run, to, a, lil, place...</td>\n",
       "      <td>fb-valence-arousal-anon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199002</th>\n",
       "      <td>Once upon a time there was a little girl calle...</td>\n",
       "      <td>[onc, upon, a, time, there, wa, a, littl, girl...</td>\n",
       "      <td>tales-emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213773</th>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "      <td>[dure, the, period, of, fall, in, love, each, ...</td>\n",
       "      <td>isear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  ... surprise\n",
       "0             Remember what she said in my last letter? \"  ...      NaN\n",
       "10062   @PressSec sounds so defensive  &amp; combative...  ...      NaN\n",
       "12647   @tiffanylue i know  i was listenin to bad habi...  ...      0.0\n",
       "52387   I'm a #Republican, but if I have to hear my mo...  ...      0.0\n",
       "56443   Thinks that @melbahughes had a great 50th birt...  ...      1.0\n",
       "77494              Mortar assault leaves at least 18 dead  ...      0.0\n",
       "78744   He who exalts himself shall      be humbled; a...  ...      0.0\n",
       "83612   This did the trick : the boys now have a more ...  ...      0.0\n",
       "86026                                The kitchen stinks .  ...      0.0\n",
       "189005  @ZubairSabirPTI  pls dont insult the word 'Molna'  ...      NaN\n",
       "196107  We'll be off and running to a lil' place calle...  ...      NaN\n",
       "199002  Once upon a time there was a little girl calle...  ...      0.0\n",
       "213773  During the period of falling in love, each tim...  ...      NaN\n",
       "\n",
       "[13 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = pickle.load(open(\"dataset/organized_dataset.pkl\", \"rb\"))\n",
    "\n",
    "df = pd.DataFrame(datasets, columns = ['text', 'tokens', 'source', 'label', 'anger','disgust','fear', 'joy', 'love', 'noemo', 'sadness', 'surprise']) \n",
    "df.drop_duplicates(['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1907986,
     "status": "error",
     "timestamp": 1589380919292,
     "user": {
      "displayName": "Moisés Oliveira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi3zXARzg3mEYgGoqoQsflEXnzm_wT1KuBZBsB37A=s64",
      "userId": "12199513049896561840"
     },
     "user_tz": 180
    },
    "id": "R8nz2jCVnDey",
    "outputId": "f562e00e-ee8d-4470-c31e-7822104b14f0"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-1875bff88fbb>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    w2v = pickle.load(open(\"w2v_unified\"+str(size_word2vec)\".pkl\", \"rb\"))\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "size_word2vec = 300\n",
    "vectors_name = \"ft_unified\"+str(size_word2vec)+\".pkl\"\n",
    "try:\n",
    "    w2v = pickle.load(open(vectors_name, \"rb\"))\n",
    "except:\n",
    "    w2v = gensim.models.Word2Vec(df['tokens'].values,size=size_word2vec, workers=8, compute_loss=True, )\n",
    "    save_object(w2v, vectors_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFE-k-1BdqNU"
   },
   "outputs": [],
   "source": [
    "training_loss = w2v.get_latest_training_loss()\n",
    "print(training_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOjrGEFNmbeE"
   },
   "outputs": [],
   "source": [
    "print(w2v.wv.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print(w2v.wv.most_similar('gr8'))\n",
    "print(w2v.wv.most_similar('newyork'))\n",
    "print(w2v.wv.most_similar('twitter_usernam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl2JZR2YowtG"
   },
   "outputs": [],
   "source": [
    "crowdflower = df.loc[df['source'].isin(['dailydialog', 'crowdflower', 'electoraltweets'])]\n",
    "crowdflower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbIS3VrRZok7"
   },
   "outputs": [],
   "source": [
    "labels = ['anger'] #, 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "crowdflower[labels].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDrUsjI6pRYQ"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def getIndex(t):\n",
    "    try:\n",
    "        return w2v.wv.vocab[t].index\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "sequences = [[getIndex(t) for t in comment]\n",
    "             for comment in crowdflower['tokens']]\n",
    "\n",
    "maxlen = MAX_SEQUENCE_LENGTH = 35\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "y = crowdflower[labels].values\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "# y = np.reshape(y, [len(labels), len(y)])\n",
    "# y_train.shape\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.20, random_state=42)\n",
    "# y_train = np.reshape(y_train, [len(labels), len(y_train)])\n",
    "# y_train.shape\n",
    "\n",
    "# y_test = np.reshape(y_test, [len(labels), len(y_test)])\n",
    "# y_test.shape\n",
    "MAX_NB_WORDS = len(w2v.wv.vocab)\n",
    "nb_words = min(MAX_NB_WORDS, len(w2v.wv.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjtxezJ_CQQq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "MAX_NB_WORDS = len(w2v.wv.vocab)\n",
    "nb_words = min(MAX_NB_WORDS, len(w2v.wv.vocab))\n",
    "\n",
    "def model_gru(embedding_matrix, embed_size):\n",
    "    outp = list()\n",
    "    inp = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    x = tf.keras.layers.Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "\n",
    "    for l in labels:\n",
    "        layers['x'+ l]= tf.keras.layers.Bidirectional(tf.keras.layers.GRU(35, return_sequences=True))(x)\n",
    "        layers['avg_pool'+ l] = tf.keras.layers.GlobalAveragePooling1D()(layers['x'+ l])\n",
    "        layers['max_pool'+ l] = tf.keras.layers.GlobalMaxPooling1D()(layers['x'+ l])\n",
    "        layers['conc'+ l] = tf.keras.layers.concatenate([layers['avg_pool'+ l], layers['max_pool'+ l]])\n",
    "        layers['conc'+ l] = tf.keras.layers.Dense(70, activation=\"relu\")(layers['conc'+ l])\n",
    "\n",
    "        layers['conc'+ l] = tf.keras.layers.Dropout(0.5)(layers['conc'+ l])\n",
    "        outp.append(tf.keras.layers.Dense(1, activation=\"sigmoid\")(layers['conc'+ l]))\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_gru(w2v.wv.vectors, 300)\n",
    "\n",
    "tf.keras.utils.plot_model(model,show_shapes=True, expand_nested=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT21i79-f9Hn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def balanced_batch_generator(x, y, batch_size, categorical=True):\n",
    "    \"\"\"A generator for creating balanced batched.\n",
    "\n",
    "    This generator loops over its data indefinitely and yields balanced,\n",
    "    shuffled batches.\n",
    "\n",
    "    Args:\n",
    "    x (numpy.ndarray): Samples (inputs). Must have the same length as `y`.\n",
    "    y (numpy.ndarray): Labels (targets). Must be a binary class matrix (i.e.,\n",
    "        shape `(num_samples, num_classes)`). You can use `keras.utils.to_categorical`\n",
    "        to convert a class vector to a binary class matrix.\n",
    "    batch_size (int): Batch size.\n",
    "    categorical (bool, optional): If true, generates binary class matrices\n",
    "        (i.e., shape `(num_samples, num_classes)`) for batch labels (targets).\n",
    "        Otherwise, generates class vectors (i.e., shape `(num_samples, )`).\n",
    "        Defaults to `True`.\n",
    "    Returns a generator yielding batches as tuples `(inputs, targets)` that can\n",
    "        be directly used with Keras.\n",
    "    \"\"\"\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError('Args `x` and `y` must have the same length.')\n",
    "    if len(y.shape) != 2:\n",
    "        raise ValueError(\n",
    "            'Arg `y` must have a shape of (num_samples, num_classes). ' +\n",
    "            'You can use `keras.utils.to_categorical` to convert a class vector ' +\n",
    "            'to a binary class matrix.'\n",
    "        )\n",
    "    if batch_size < 1:\n",
    "        raise ValueError('Arg `batch_size` must be a positive integer.')\n",
    "    num_samples = y.shape[0]\n",
    "    num_classes = y.shape[1]\n",
    "    batch_x_shape = (batch_size, *x.shape[1:])\n",
    "    batch_y_shape = (batch_size, num_classes) if categorical else (batch_size, )\n",
    "    indexes = [0 for _ in range(num_classes)]\n",
    "    samples = [[] for _ in range(num_classes)]\n",
    "    for i in range(num_samples):\n",
    "        samples[np.argmax(y[i])].append(x[i])\n",
    "    while True:\n",
    "        batch_x = np.ndarray(shape=batch_x_shape, dtype=x.dtype)\n",
    "        batch_y = np.zeros(shape=batch_y_shape, dtype=y.dtype)\n",
    "        for i in range(batch_size):\n",
    "            random_class = random.randrange(num_classes)\n",
    "            current_index = indexes[random_class]\n",
    "            indexes[random_class] = (current_index + 1) % len(samples[random_class])\n",
    "            if current_index == 0:\n",
    "                random.shuffle(samples[random_class])\n",
    "            batch_x[i] = samples[random_class][current_index]\n",
    "            if categorical:\n",
    "                batch_y[i][random_class] = 1\n",
    "            else:\n",
    "                batch_y[i] = random_class\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIh_9k9ruAaV"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# define the undersampling method\n",
    "undersample = NearMiss(version=1, n_neighbors=3)\n",
    "# transform the dataset\n",
    "x_test, y_test = undersample.fit_resample(x_test, y_test)\n",
    "\n",
    "\n",
    "# define the undersampling method\n",
    "undersample = NearMiss(version=1, n_neighbors=3)\n",
    "# transform the dataset\n",
    "x_train, y_train = undersample.fit_resample(x_train, y_train)\n",
    "'''\n",
    "hist = model.fit_generator(generator=balanced_batch_generator(x_train, y_train,20),\n",
    "                            steps_per_epoch=1,\n",
    "                            validation_data = (x_test, y_test),\n",
    "                            epochs=20, verbose=1)\n",
    "'''\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=10,\n",
    "                    batch_size=30,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWlKltzaF6_q"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR5AXYKJWGBh"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4GglzWfwbud"
   },
   "outputs": [],
   "source": [
    "save_object(model, 'convlsdtmw2v50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PMZ5al5PLdg"
   },
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"convlsdtmw2v50.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANdYk6VwN_-Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# rus = RandomUnderSampler(return_indices=True)\n",
    "# x_rus, y_rus, id_rus = rus.fit_sample(x_test, y_test)\n",
    "\n",
    "# print('Removed indexes:', id_rus)\n",
    "predictions = model.predict(x_test)\n",
    "y_pred = (predictions > 0.5)\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "matrix = matrix/matrix.astype(np.float).sum(axis=1)\n",
    "sns.heatmap(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VybGncilrQcQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], \n",
    "            loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history[MAX_NB_WORDS = len(w2v.wv.vocab)\n",
    "'loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibZojLwKZZEw"
   },
   "outputs": [],
   "source": [
    " # More metrics\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# https://nbviewer.jupyter.org/github/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20with%20Keras%20for%20the%20Iris%20Prediction.ipynb#seven\n",
    "\n",
    "labels[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwC5QDPCS6ymT2ARMuX9TI",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "1 - preprocessing",
   "provenance": []
  },
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
